{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coregistration of two Sentinel-2 scenes with a Landsat-8 reference scene <img align=\"right\" src=\"../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* **[Sign up to the DEA Sandbox](https://app.sandbox.dea.ga.gov.au/)** to run this notebook interactively from a browser\n",
    "* **Compatibility:** Notebook currently compatible with `DEA Sandbox` environments\n",
    "* **Products used:** \n",
    "[landsat-c2l2-sr](https://landsatlook.usgs.gov/stac-server/collections/landsat-c2l2-sr)\n",
    "[sentinel-2-l2a](https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Satellite image co-registration ensures the accurate geometric alignment of multi-temporal or multi-sensor images. It is critical for applications like change detection, data fusion, and displacement mapping. Co-registration of satellite images is a critical process in remote sensing, enabling accurate multi-temporal and multi-sensoral analysis. This process aligns images taken at different times or by different sensors to a common coordinate system, reducing misalignments caused by geometric errors. For more details on co-registration and its available tools please see the references below:\n",
    "\n",
    "[Satelite Image Co-Registration](https://geoscienceau.sharepoint.com/:b:/s/DEAnt/ERAjz6UbYLJGh20wvObq2YwBZ-QNrGk37Qz5ObjLQNJQfA?e=lKubfl)\n",
    "\n",
    "[Co-Registration Tools Comparison](https://geoscienceau.sharepoint.com/:b:/s/DEAnt/Efz-tYil3pVDv3nrvMIYBHoBlZbvUwKTc1aOOpVYPPX7Ig?e=0hMtWp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebooks tests a optical images cor-registeration of two Sentinel-2 target scenese with a Landsat-8 reference one, using three different tools and compares their results. `Co-register` , `Karios` and `AROSICS`. The first two are mainly based on Optical Flow tracking of the features in the scenes and the last one mainly uses Phase Correlation. `Co-register` is built in-house in GA and is compared with the other two. \n",
    "\n",
    "For more information about Karios ansd AROSICS, refer to the links below:\n",
    "\n",
    "[Karios](https://github.com/telespazio-tim/karios)\n",
    "\n",
    "[AROSICS](https://github.com/GFZ/arosics)\n",
    "\n",
    "1. First we download and prepare the reference and target scenes.\n",
    "2. Second, we run three different co-registration tools using the input scenes.\n",
    "3. Finally we compare the results using the available metrics for image similarity.\n",
    "\n",
    ">**Note:** \n",
    ">* All parameters are defualt parameters, except for AROSICS, where the min reliability was dropped to 30%.\n",
    ">* Shifts are translation shifts only. Mean shifts (Average shifts in both x and y directions from all detected features) are used for final co-registration.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "AROSICS and KArios should be installed in your environment. \n",
    "\n",
    "[AROSICS intallation](https://danschef.git-pages.gfz-potsdam.de/arosics/doc/installation.html)\n",
    "\n",
    "For Karios, you can follow the link below:\n",
    "\n",
    "[Karios installation](https://github.com/telespazio-tim/karios?tab=readme-ov-file#installation)\n",
    "\n",
    "or you can just install from the main branch of the repo by running:\n",
    "\n",
    "\n",
    "```pip install git+https://github.com/telespazio-tim/karios.git@main```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pystac_client import Client\n",
    "import boto3\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import geopandas as gpd\n",
    "from skimage.exposure import rescale_intensity\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from rasterio.plot import show\n",
    "import planetary_computer\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(1, \"../Tools/\")\n",
    "from utils import reproject_tif, stream_scene, co_register, arosics, karios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying data using PyStac\n",
    "\n",
    "We use landsat-8 with scene id `LC08_L2SR_128111_20200228_02_T2` as the reference image and Sentinel-2 scenes with ids `S2B_42CVE_20250124_0_L2A`, `S2B_42CVE_20250110_0_L2A`, as target or monitored scenes to be co-registered with the refernce scene. These scenes are all from the Amery ice shelf in Antarctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ids = [\"LC08_L2SR_128111_20200228_02_T2\"]\n",
    "tgt_ids = [\"S2B_42CVE_20250124_0_L2A\", \"S2B_42CVE_20250110_0_L2A\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input and outpur folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dirs = \"temp_dea_coreg/inputs\"\n",
    "output_dirs = \"temp_dea_coreg/outputs\"\n",
    "os.makedirs(input_dirs, exist_ok=True)\n",
    "os.makedirs(output_dirs, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-registration can be run only on single bands or on multi-band images. For this test we use an averaged greyscale image of a RGB composite of 3 seperate bands `red`, `green` and `blue`, with contrast stretching as input image pre-enhancement step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [\"SR_B4\", \"SR_B3\", \"SR_B2\"]  # specifying the bands to be used for RGB composite\n",
    "# bands = [\"red\", \"green\", \"blue\"]  # specifying the bands to be used for RGB composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying and downloading the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planetary Computer assets are held in private Azure blobs. We need to sign the assets to access them. This is done via the right modifier passed to pystac client.\n",
    "server_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "query = {\"ids\": ref_ids, \"collections\": [\"landsat-8-c2-l2\"]}\n",
    "client = Client.open(server_url, modifier=planetary_computer.sign_inplace)\n",
    "search = client.search(**query)\n",
    "ref_items = search.item_collection()\n",
    "print(len(ref_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # querying the reference scene using PyStac\n",
    "# server_url = \"https://landsatlook.usgs.gov/stac-server\"\n",
    "# query = {\"ids\": ref_ids}\n",
    "# client = Client.open(server_url, headers=headers)\n",
    "# search = client.search(**query)\n",
    "# ref_items = search.item_collection()\n",
    "# display(ref_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the reference scene bands from the cloud using rasterio.\n",
    "\n",
    "Landsat-8 data on AWS is stored in a requester-pays bucket, so we need to create a session that supports this if downloading from AWS. Planetary Computer scenes are hosted on Azure. The Urls for them need to be signed before we can download them. We have already signed the assets in place when opening the pystac client.\n",
    "\n",
    "`stream_scene` function is defined in `tools.coregister` library , and it returns the data as a numpy array and its metadata. We can scale the data while streaming to reduce memory usage. Here we set the output resolution to 200.0 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_data = []  # stores data for each band\n",
    "for band in bands:\n",
    "    band_url = ref_items[0].assets[band].href\n",
    "    data = stream_scene(band_url, resolution=200.0, round_transform=False)\n",
    "    bands_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws_session = rio.session.AWSSession(boto3.Session(), requester_pays=True)\n",
    "# bands_data = []  # stores data for each band\n",
    "# for band in bands:\n",
    "#     band_url = ref_items[0].assets[band].to_dict()[\"alternate\"][\"s3\"][\"href\"]\n",
    "#     data = stream_scene(\n",
    "#         band_url, aws_session=aws_session, resolution=200.0, round_transform=False\n",
    "#     )\n",
    "#     bands_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the bands into a single RGB image\n",
    "bands_images = [np.nan_to_num(np.squeeze(datum[0])) for datum in bands_data]\n",
    "composite_image = cv.merge(bands_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the composite image for inspection\n",
    "plt.imshow(composite_image / composite_image.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "We now create a grayscale image and apply contrast enhancement on it to create a more informative reference image. \n",
    "Karios and AROSICS work on a single band only (even if the input is a multi-band image), therefore we use the code below to create a grey reference image by averaging the bands in the true colour image. Please note, this is not the usual grayscale image that image processing tools such as OpenCV generate. The normal conversion is according NTSC formula which preserves the relationship between bands to align better with human perception. \n",
    "Also we stretch the contrast of the composite images. This will reveal more information about the features in the images, potentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a grayscale version of the reference image for use in registration\n",
    "# using the 2nd and 98th percentiles to stretch the contrast for image enhancement\n",
    "# and saving it to disk\n",
    "\n",
    "grey_image = np.mean(composite_image, axis=2)\n",
    "p2, p98 = np.percentile(grey_image, (0, 98))\n",
    "ref_grey = rescale_intensity(grey_image, in_range=(p2, p98), out_range=\"uint8\")\n",
    "ref_profile = bands_data[0][1][\"profile\"]\n",
    "ref_profile[\"dtype\"] = \"uint8\"\n",
    "with rio.open(f\"{input_dirs}/ref_image.tif\", \"w\", **ref_profile) as dst:\n",
    "    dst.write(ref_grey, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dwonloading and processing the target scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_url = \"https://earth-search.aws.element84.com/v1\"\n",
    "query = {\"ids\": tgt_ids}\n",
    "client = Client.open(server_url)\n",
    "search = client.search(**query)\n",
    "tgt_items = search.item_collection()\n",
    "print(len(tgt_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [\"red\", \"green\", \"blue\"]  # specifying the bands to be used for RGB composite\n",
    "aws_session = rio.session.AWSSession(boto3.Session())\n",
    "items_data = []\n",
    "for item in tgt_items:\n",
    "    print(item.id)\n",
    "    bands_data = []\n",
    "    for band in bands:\n",
    "        band_url = item.assets[band].href\n",
    "        data = stream_scene(\n",
    "            band_url, aws_session=aws_session, resolution=200.0, round_transform=False\n",
    "        )\n",
    "        bands_data.append(data)\n",
    "    items_data.append({\"id\": item.id, \"bands\": bands_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_composite_images = []\n",
    "for idx, item_data in enumerate(items_data):\n",
    "    bands_data = item_data[\"bands\"]\n",
    "    bands_images = [np.nan_to_num(np.squeeze(datum[0])) for datum in bands_data]\n",
    "    composite_image = cv.merge(bands_images)\n",
    "    tgt_composite_images.append(composite_image)\n",
    "\n",
    "plt.imshow(tgt_composite_images[0] / tgt_composite_images[0].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the targets to grayscale images and applying contrast enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grayscale target images\n",
    "\n",
    "tgt_greys = []\n",
    "for idx, composite_image in enumerate(tgt_composite_images):\n",
    "    grey_image = np.mean(composite_image, axis=2)\n",
    "    p2, p98 = np.percentile(grey_image, (0, 98))\n",
    "    tgt_image = rescale_intensity(grey_image, in_range=(p2, p98), out_range=\"uint8\")\n",
    "    tgt_greys.append(tgt_image)\n",
    "    tgt_profile = items_data[idx][\"bands\"][0][1][\"profile\"]\n",
    "    tgt_profile[\"dtype\"] = \"uint8\"\n",
    "    with rio.open(f\"{input_dirs}/tgt_image_{idx}.tif\", \"w\", **tgt_profile) as dst:\n",
    "        dst.write(tgt_image, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the reference and target images next to each  other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 36), dpi=200)\n",
    "show(\n",
    "    rio.open(f\"{input_dirs}/ref_image.tif\"),\n",
    "    ax=axes[0],\n",
    "    title=\"Reference Image\",\n",
    "    cmap=\"gray\",\n",
    ")\n",
    "for idx in range(len(tgt_greys)):\n",
    "    show(\n",
    "        rio.open(f\"{input_dirs}/tgt_image_{idx}.tif\"),\n",
    "        ax=axes[idx + 1],\n",
    "        title=f\"Target Image {idx}\",\n",
    "        cmap=\"gray\",\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data exploration using geopandas\n",
    "gdf = gpd.GeoDataFrame.from_features(ref_items + tgt_items)\n",
    "ids = [item.id for item in ref_items + tgt_items]\n",
    "gdf[\"id\"] = ids\n",
    "gdf.plot(\n",
    "    column=\"id\",\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    "    legend=True,\n",
    "    figsize=(10, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the file names into input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_file = f\"{input_dirs}/ref_image.tif\"\n",
    "tgt_files = glob.glob(f\"{input_dirs}/tgt_image_*.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rio.open(ref_file).crs[\"proj\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprojecting targets\n",
    "\n",
    "Reference file has polar stereographic projection (Landsat-8), whereas the target Sentinel-2 files are in UTM coordinates. We must reproject the targets to the reference coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject_tif function from tools.coregister library reprojects a tif file to a specified CRS\n",
    "ref_epsg = rio.open(\n",
    "    ref_file\n",
    ").crs.to_epsg()  # getting the EPSG code of the reference image\n",
    "for i, tgt in enumerate(tgt_files):\n",
    "    tgt_epsg = rio.open(tgt).crs.to_epsg()\n",
    "    print(f\"Target image {i} EPSG: {tgt_epsg}\")\n",
    "    if tgt_epsg != ref_epsg:\n",
    "        reproj_dir = os.path.join(input_dirs, \"reprojected\")\n",
    "        os.makedirs(reproj_dir, exist_ok=True)\n",
    "        print(f\"Reprojecting target image {i} to match reference EPSG {ref_epsg}...\")\n",
    "        reprojected_tgt = os.path.join(reproj_dir, os.path.basename(tgt))\n",
    "        reproject_tif(tgt, reprojected_tgt, rio.open(ref_file).crs)\n",
    "        tgt_files[i] = reprojected_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Co-Registration tools\n",
    "\n",
    "we will run `co-register`, `karios` and `arosics` functions from the `tools.coregister` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-Register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{output_dirs}/Co_Register\"\n",
    "\n",
    "_, shifts, target_ids = co_register(\n",
    "    ref_file,\n",
    "    tgt_files,\n",
    "    output_path=output_path,\n",
    "    return_shifted_images=True,\n",
    "    laplacian_kernel_size=5,\n",
    ")\n",
    "print(\"\\nCo-register shifts:\")\n",
    "for i, shift in enumerate(shifts):\n",
    "    print(\n",
    "        f\"Target {target_ids[i]}: {tuple([np.round(el.tolist(), 3).tolist() for el in shift])} pixels\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Karios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = output_path = f\"{output_dirs}/Karios\"\n",
    "shift_dict, target_ids = karios(\n",
    "    ref_file,\n",
    "    tgt_files,\n",
    "    output_dir,\n",
    ")\n",
    "print(\"\\nKarios shifts:\")\n",
    "for i, shift in enumerate(shift_dict):\n",
    "    print(\n",
    "        f\"Target {target_ids[i]}: {tuple([np.round(el, 3).tolist() for el in shift_dict[shift]])} pixels\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AROSICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = output_path = f\"{output_dirs}/Arosics\"\n",
    "shifts, target_ids = arosics(\n",
    "    ref_file,\n",
    "    tgt_files,\n",
    "    output_dir,\n",
    ")\n",
    "print(\"\\nAROSICS shifts:\")\n",
    "for i, shift in enumerate(shifts):\n",
    "    print(\n",
    "        f\"Target {target_ids[i]}: {tuple([np.round(el.tolist(), 3).tolist() for el in shift])} pixels\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Results \n",
    "\n",
    "Output gifs are generated in the outputs folder under each tool's name. `outputs_raw.gif` is the file generated from the raw inputs before co-registration. `outputs.gif` is the fianl gif generated after co-registration. `output.csv` is the generated csv file for the performance metrics and other reported values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_subdirs = [\"Co_Register\", \"Karios\", \"Arosics\"]\n",
    "output_pd = pd.DataFrame({\"Title\": [\"target 0\", \"target 1\"]})\n",
    "for i, output_subdir in enumerate(output_subdirs):\n",
    "    output_csv = f\"{output_dirs}/{output_subdir}/output.csv\"\n",
    "    temp_pd = pd.read_csv(output_csv)\n",
    "    if i == 0:\n",
    "        output_pd[\"SSIM Raw\"] = temp_pd[\"SSIM Raw\"]\n",
    "        output_pd[\"MSE Raw\"] = temp_pd[\"MSE Raw\"]\n",
    "    output_pd[f\"{output_subdir} SSIM\"] = temp_pd[\"SSIM Aligned\"]\n",
    "    output_pd[f\"{output_subdir} MSE\"] = temp_pd[\"MSE Aligned\"]\n",
    "    output_pd[f\"{output_subdir} Shifts\"] = temp_pd[\"Shifts\"]\n",
    "display(output_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_pd = output_pd.copy()[\n",
    "    [\"Title\"] + [f\"{subdir} Shifts\" for subdir in output_subdirs]\n",
    "]\n",
    "for subdir in output_subdirs:\n",
    "    shifts_pd[f\"{subdir} Shifts x\"] = shifts_pd[f\"{subdir} Shifts\"].apply(\n",
    "        lambda x: float(x.split(\",\")[0].replace(\"(\", \"\").strip())\n",
    "    )\n",
    "    shifts_pd[f\"{subdir} Shifts y\"] = shifts_pd[f\"{subdir} Shifts\"].apply(\n",
    "        lambda x: float(x.split(\",\")[1].replace(\")\", \"\").strip())\n",
    "    )\n",
    "shifts_pd.drop(columns=[f\"{subdir} Shifts\" for subdir in output_subdirs], inplace=True)\n",
    "shifts_pd[[\"Title\"] + [f\"{subdir} Shifts x\" for subdir in output_subdirs]].plot(\n",
    "    kind=\"bar\", x=\"Title\", figsize=(10, 6), title=\"Shifts in x in pixels\"\n",
    ")\n",
    "shifts_pd[[\"Title\"] + [f\"{subdir} Shifts y\" for subdir in output_subdirs]].plot(\n",
    "    kind=\"bar\", x=\"Title\", figsize=(10, 6), title=\"Shifts in y in pixels\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Discord chat](https://discord.com/invite/4hhBQVas5U) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [GitHub](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** September 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "<!-- Browse all available tags on the DEA User Guide's [Tags Index](https://knowledge.dea.ga.gov.au/genindex/) -->"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Tags**: :index:`co-register`, :index:`karios`, :index:`arosics`, :index:`sandbox compatible`, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
