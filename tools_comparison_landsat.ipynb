{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c1a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import boto3\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "aws_session = rasterio.session.AWSSession(boto3.Session(), requester_pays=True)\n",
    "\n",
    "dir_suffix = \"\"\n",
    "\n",
    "aoi_index = -1\n",
    "\n",
    "keep_original_band_scenes = False\n",
    "one_per_month = True\n",
    "remove_duplicate_times = True\n",
    "query_and_process = True\n",
    "\n",
    "force_path_row = True\n",
    "forced_pr = \"089080\"\n",
    "\n",
    "fill_nodata = False\n",
    "\n",
    "if force_path_row:\n",
    "    one_per_month = False\n",
    "\n",
    "combined_path_rows = False\n",
    "alternate_pairs = False\n",
    "\n",
    "if not combined_path_rows:\n",
    "    alternate_pairs = False\n",
    "\n",
    "# platform = [\"LANDSAT_4\", \"LANDSAT_5\"]\n",
    "platform = \"LANDSAT_5\"\n",
    "# platform = \"LANDSAT_7\"\n",
    "# platform = \"LANDSAT_8\"\n",
    "\n",
    "reference_month = \"01\"\n",
    "reference_month_1 = \"01\"\n",
    "reference_month_2 = \"01\"\n",
    "\n",
    "enhance_image = True\n",
    "\n",
    "bands = [\"red\", \"green\", \"blue\"]  # \"swir16\"]\n",
    "\n",
    "subdir = \"rgb_enhanced\"\n",
    "\n",
    "forced_ids = None\n",
    "forced_ids = [\n",
    "    \"LT05_L2SP_089080_20040504_20200903_02_T1_SR\",\n",
    "    \"LT05_L2SP_089080_20110711_20200822_02_T2_SR\",\n",
    "    \"LT05_L2SP_089080_20111015_20200820_02_T1_SR\",\n",
    "]\n",
    "\n",
    "id_filter = \"\"\n",
    "if platform in [\"LANDSAT_4\", \"LANDSAT_5\"]:\n",
    "    if \"_SR\" in id_filter:\n",
    "        l_4_5_collections = [\"landsat-c2l2-sr\"]\n",
    "    else:\n",
    "        l_4_5_collections = [\"landsat-c2l1\"]\n",
    "    dir_suffix = id_filter.replace(\"_\", \"\") + dir_suffix\n",
    "\n",
    "outputs_folder = (\n",
    "    f\"outputs_coreg/outputs_RGB_enhanced{'_infilled' if fill_nodata else ''}\"\n",
    ")\n",
    "\n",
    "force_reprocess = False\n",
    "\n",
    "filename_suffix = \"PROC\"\n",
    "\n",
    "inputs_dir = \"inputs_coreg\"\n",
    "\n",
    "if (dir_suffix != \"\") and (not dir_suffix.endswith(\"/\")):\n",
    "    dir_suffix = dir_suffix + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr_dir = [\n",
    "#     dir\n",
    "#     for dir in glob.glob(\n",
    "#         \"data/inputs_old_coreg_outputs/outputs_coreg/outputs_RGB_enhanced/*\"\n",
    "#     )\n",
    "#     if forced_pr in dir\n",
    "# ][0]\n",
    "# landsat_dirnames = glob.glob(f\"{pr_dir}/Karios/LE*\")\n",
    "# ref_name = landsat_dirnames[0].split(\"PROC\")[1][1:-1]\n",
    "# tgt_0_name = os.path.basename(landsat_dirnames[0].split(\"PROC\")[0][:-1])\n",
    "# tgt_1_name = os.path.basename(landsat_dirnames[1].split(\"PROC\")[0][:-1])\n",
    "# forced_ids = [ref_name, tgt_0_name, tgt_1_name]\n",
    "# print(\"Forced IDs:\", forced_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_bbox = resize_bbox(BoundingBox(*kml_to_poly(\"data/inputs_old/WA.kml\").bounds), 0.1)\n",
    "aoi_polys = kml_to_poly(\"data/inputs_old/aois.kml\").geoms\n",
    "\n",
    "bbox_list = [\n",
    "    [67.45, -72.55, 67.55, -72.45],  # Amery bed rock\n",
    "    [69.2, -68.1, 69.4, -67.9],  # Amery top\n",
    "    [wa_bbox.left, wa_bbox.bottom, wa_bbox.right, wa_bbox.top],  # WA sand dunes\n",
    "    *[list(p.bounds) for p in aoi_polys],  # AOI polygons\n",
    "    list(\n",
    "        resize_bbox(BoundingBox(*kml_to_poly(\"data/inputs_old/TAS.kml\").bounds), 0.1)\n",
    "    ),  # TAS\n",
    "    [152.12, -28.37, 154.4, -26.48],  # QLD\n",
    "]\n",
    "\n",
    "print(\"Using AOI index:\", aoi_index)\n",
    "print(\"Using AOI bbox:\")\n",
    "print([np.round(bb, 2).tolist() for bb in bbox_list[aoi_index]])\n",
    "\n",
    "query_platform = platform\n",
    "if type(platform) is list:\n",
    "    platform = (\n",
    "        platform[0].split(\"_\")[0] + \"_\" + \"_\".join(platform).replace(\"LANDSAT_\", \"\")\n",
    "    )\n",
    "    print(\"Using platform:\", platform)\n",
    "\n",
    "if force_path_row:\n",
    "    if forced_ids is not None:\n",
    "        extra_query = {\"ids\": forced_ids}\n",
    "        print(\"Using extra query:\", extra_query)\n",
    "    else:\n",
    "        scene_df_file = (\n",
    "            f\"data/{inputs_dir}/{dir_suffix}{platform}_{forced_pr}/pairs.csv\"\n",
    "        )\n",
    "        if os.path.exists(scene_df_file):\n",
    "            scene_df = pd.read_csv(scene_df_file)\n",
    "            scene_ids = [\n",
    "                os.path.basename(s).replace(\"_TC.TIF\", \"\").replace(\"_PROC.TIF\", \"\")\n",
    "                for s in scene_df.iloc[0, :].tolist()\n",
    "            ]\n",
    "            extra_query = {\"ids\": scene_ids}\n",
    "            print(\"Using extra query:\", extra_query)\n",
    "        else:\n",
    "            extra_query = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_and_process:\n",
    "    if platform == \"LANDSAT_8\":\n",
    "        query = get_search_query(\n",
    "            bbox_list[aoi_index] if forced_ids is None else None,\n",
    "            # start_date=\"\",\n",
    "            start_date=\"2013-01-01T00:00:00\",\n",
    "            end_date=\"2017-01-01T00:00:00\",\n",
    "            platform=query_platform,\n",
    "            collection_category=None,\n",
    "            collections=[\"landsat-c2l2-sr\"],\n",
    "            extra_query=extra_query if force_path_row else None,\n",
    "            cloud_cover=5,\n",
    "        )\n",
    "    elif platform == \"LANDSAT_7\":\n",
    "        query = get_search_query(\n",
    "            bbox_list[aoi_index] if forced_ids is None else None,\n",
    "            # start_date=\"\",\n",
    "            start_date=\"2003-01-01T00:00:00\",\n",
    "            end_date=\"2005-12-31T00:00:00\",\n",
    "            platform=query_platform,\n",
    "            collection_category=None,\n",
    "            collections=[\"landsat-c2l2-sr\"],\n",
    "            extra_query=extra_query if force_path_row else None,\n",
    "            # cloud_cover=50,\n",
    "        )\n",
    "    else:  # LANDSAT 4, 5\n",
    "        query = get_search_query(\n",
    "            bbox_list[aoi_index] if forced_ids is None else None,\n",
    "            start_date=\"2003-12-31T00:00:00\",\n",
    "            # start_date=\"1985-01-01T00:00:00\",\n",
    "            end_date=\"2012-12-31T00:00:00\",\n",
    "            platform=query_platform,\n",
    "            collection_category=None,\n",
    "            collections=l_4_5_collections,\n",
    "            cloud_cover=5,\n",
    "            extra_query=extra_query if force_path_row else None,\n",
    "        )\n",
    "\n",
    "    # if force_path_row:\n",
    "    #     del query[\"bbox\"]\n",
    "\n",
    "    print(\"Search query:\", query)\n",
    "\n",
    "    server_url = \"https://landsatlook.usgs.gov/stac-server/search\"\n",
    "    features = query_stac_server(query, server_url, id_filter=id_filter)\n",
    "    print(len(features), \"features found\")\n",
    "\n",
    "    if len(features) < 12 and not force_path_row:\n",
    "        print(f\"Not enough features found: {len(features)}, skipping AOI {aoi_index}\")\n",
    "    else:\n",
    "        os.makedirs(f\"data/{inputs_dir}/features/{platform}\", exist_ok=True)\n",
    "        with open(\n",
    "            f\"data/{inputs_dir}/features/{platform}/\" + str(aoi_index) + \".pkl\", \"wb\"\n",
    "        ) as f:\n",
    "            pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c472208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not query_and_process:\n",
    "#     with open(f\"data/inputs/features/{platform}/\" + str(aoi_index) + \".pkl\", \"rb\") as f:\n",
    "#         features = pickle.load(f)\n",
    "\n",
    "if query_and_process:\n",
    "    scene_dict, scene_list = find_scenes_dict(\n",
    "        features,\n",
    "        one_per_month=one_per_month,\n",
    "        # start_end_years=[2009, 2010],\n",
    "        acceptance_list=bands + [\"thumbnail\"],\n",
    "        remove_duplicate_times=remove_duplicate_times,\n",
    "        duplicate_idx=1,\n",
    "    )\n",
    "    path_rows = list(scene_dict.keys())\n",
    "    if len(path_rows) == 0:\n",
    "        raise ValueError(\"No scenes found, cannot continue\")\n",
    "    print(path_rows)\n",
    "    dates = [list(scene_dict[pr].keys()) for pr in path_rows]\n",
    "    date_len = [len(d) for d in dates]\n",
    "\n",
    "    path_row = path_rows[np.argmax(date_len)]\n",
    "    diffs = [abs(int(pr) - int(path_row)) for pr in path_rows]\n",
    "    up = path_rows[np.argmax(diffs)]\n",
    "    pr_list = [path_row, up]\n",
    "\n",
    "    print(path_row)\n",
    "    print(pr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403fc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_scene_dict, full_scene_list = find_scenes_dict(\n",
    "#     features,\n",
    "#     one_per_month=False,\n",
    "#     # start_end_years=[2009, 2010],\n",
    "#     acceptance_list=bands + [\"thumbnail\"],\n",
    "#     remove_duplicate_times=False,\n",
    "#     duplicate_idx=1,\n",
    "# )\n",
    "# full_scene_list = [f for f in full_scene_list if \"089080\" in f[\"scene_name\"]]\n",
    "# print(len(full_scene_list), \"scenes in full list\")\n",
    "# pr_date_list_processed = download_and_process_series(\n",
    "#     full_scene_list,\n",
    "#     bands,\n",
    "#     [\"_B3\", \"_B2\", \"_B1\"],\n",
    "#     \"templ5/l5_input\",\n",
    "#     \"templ5/l5_process\",\n",
    "#     \"templ5/l5_ds\",\n",
    "#     aws_session,\n",
    "#     keep_original_band_scenes,\n",
    "#     stretch_contrast=True,\n",
    "#     gray_scale=True,\n",
    "#     preserve_depth=True,\n",
    "#     min_max_scaling=False,\n",
    "# )\n",
    "# files = sorted(glob.glob(\"templ5/l5_ds/*\"))\n",
    "# make_difference_gif(\n",
    "#     files,\n",
    "#     \"templ5/out_3.gif\",\n",
    "#     mosaic_scenes=True,\n",
    "#     fps=10,\n",
    "# )\n",
    "# files = sorted(glob.glob(\"templ5/l5_ds/*\"))\n",
    "# methods = [\"Co_Register\", \"Arosics\", \"Karios\"]\n",
    "# for method in methods:\n",
    "#     output_dir = f\"templ5/{method}_L2T1\"\n",
    "#     coreg(\n",
    "#         files[0],\n",
    "#         files[1:],\n",
    "#         output_dir,\n",
    "#         method,\n",
    "#         fps=10,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_and_process:\n",
    "    if len(features) > 25:\n",
    "        random.seed(42)\n",
    "        features = random.sample(features, 25)\n",
    "        print(\"Randomly sampled 25 features for download\")\n",
    "\n",
    "    _, full_scene_list = find_scenes_dict(\n",
    "        features,\n",
    "        one_per_month=False,\n",
    "        # start_end_years=[2009, 2010],\n",
    "        acceptance_list=bands + [\"thumbnail\"],\n",
    "        remove_duplicate_times=False,\n",
    "        duplicate_idx=1,\n",
    "    )\n",
    "    i = 0\n",
    "    shutil.rmtree(\"temp_data\", ignore_errors=True)\n",
    "    os.makedirs(\"temp_data\", exist_ok=True)\n",
    "    s3_list = [s[\"thumbnail_alternate\"] for s in full_scene_list]\n",
    "    outputs = []\n",
    "    for url in s3_list:\n",
    "        outputs.append(f\"temp_data/{os.path.basename(url)}\")\n",
    "    bucket = \"usgs-landsat\"\n",
    "    download_files(bucket, s3_list, outputs, -1, is_async_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c28595",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = None\n",
    "if force_path_row:\n",
    "    assert forced_pr is not None, \"Forced path row must be provided\"\n",
    "    if query_and_process:\n",
    "        if len(forced_pr.split(\"_\")) > 1:\n",
    "            sn = combine_scene_dicts([scene_dict[pr] for pr in forced_pr.split(\"_\")])\n",
    "            data_dict = sn.copy()\n",
    "        else:\n",
    "            data_dict = scene_dict[forced_pr].copy()\n",
    "    pr = forced_pr\n",
    "else:\n",
    "    if combined_path_rows:\n",
    "        if query_and_process:\n",
    "            sn = combine_scene_dicts([scene_dict[pr] for pr in pr_list])\n",
    "            data_dict = sn.copy()\n",
    "        pr = \"_\".join(pr_list)\n",
    "    else:\n",
    "        if query_and_process:\n",
    "            data_dict = scene_dict[path_row].copy()\n",
    "        pr = path_row\n",
    "print(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_and_process:\n",
    "    if alternate_pairs:\n",
    "        closest_pair = get_pair_dict_alternate(\n",
    "            scene_dict[pr_list[0]],\n",
    "            scene_dict[pr_list[1]],\n",
    "            \"closest\",\n",
    "            reference_month_1=reference_month_1,\n",
    "            reference_month_2=reference_month_2,\n",
    "        )\n",
    "        farthest_pair = get_pair_dict_alternate(\n",
    "            scene_dict[pr_list[0]],\n",
    "            scene_dict[pr_list[1]],\n",
    "            \"farthest\",\n",
    "            reference_month_1=reference_month_1,\n",
    "            reference_month_2=reference_month_2,\n",
    "        )\n",
    "    else:\n",
    "        closest_pair = get_pair_dict(\n",
    "            data_dict, \"closest\", reference_month=reference_month\n",
    "        )\n",
    "        farthest_pair = get_pair_dict(\n",
    "            data_dict, \"farthest\", reference_month=reference_month\n",
    "        )\n",
    "\n",
    "    print(\"Closest pair:\")\n",
    "    print(closest_pair[0])\n",
    "    print(closest_pair[1])\n",
    "    print(\"Farthest pair:\")\n",
    "    print(farthest_pair[0])\n",
    "    print(farthest_pair[1])\n",
    "\n",
    "    s3_list = [\n",
    "        closest_pair[0][\"thumbnail_alternate\"],\n",
    "        closest_pair[1][\"thumbnail_alternate\"],\n",
    "        farthest_pair[1][\"thumbnail_alternate\"],\n",
    "    ]\n",
    "    outputs = []\n",
    "    for url in s3_list:\n",
    "        outputs.append(\n",
    "            f\"data/{inputs_dir}/thumbnails/{dir_suffix}{platform}_{path_row}/{os.path.basename(url)}\"\n",
    "        )\n",
    "\n",
    "    bucket = \"usgs-landsat\"\n",
    "    download_files(bucket, s3_list, outputs, -1, is_async_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_and_process:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    im0 = plt.imread(outputs[0])\n",
    "    im1 = plt.imread(outputs[1])\n",
    "    im2 = plt.imread(outputs[2])\n",
    "    axes[0].imshow(im0)\n",
    "    axes[1].imshow(im1)\n",
    "    axes[2].imshow(im2)\n",
    "    axes[0].set_title(os.path.basename(outputs[0]).replace(\"_thumb_small.jpeg\", \"\"))\n",
    "    axes[1].set_title(os.path.basename(outputs[1]).replace(\"_thumb_small.jpeg\", \"\"))\n",
    "    axes[2].set_title(os.path.basename(outputs[2]).replace(\"_thumb_small.jpeg\", \"\"))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"data/{inputs_dir}/{dir_suffix}{platform}_{pr}\"\n",
    "if query_and_process:\n",
    "    download_and_process_pairs(\n",
    "        (\n",
    "            [scene_dict[pr_list[0]], scene_dict[pr_list[1]]]\n",
    "            if alternate_pairs\n",
    "            else data_dict\n",
    "        ),\n",
    "        bands,\n",
    "        output_dir,\n",
    "        aws_session,\n",
    "        keep_original_band_scenes,\n",
    "        reference_month=(\n",
    "            [reference_month_1, reference_month_2]\n",
    "            if alternate_pairs\n",
    "            else reference_month\n",
    "        ),\n",
    "        gray_scale=True,\n",
    "        averaging=True,\n",
    "        subdir=subdir,\n",
    "        stretch_contrast=enhance_image,\n",
    "        force_reprocess=force_reprocess,\n",
    "        filename_suffix=filename_suffix,\n",
    "        preserve_depth=True if platform in [\"LANDSAT_4\", \"LANDSAT_5\"] else False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_df = pd.read_csv(f\"data/{inputs_dir}/{dir_suffix}{platform}_{pr}/pairs.csv\")\n",
    "ref_image = scene_df[\"Reference\"][0]\n",
    "tgt_images = [\n",
    "    scene_df[\"Closest_target\"][0],\n",
    "    scene_df[\"Farthest_target\"][0],\n",
    "]\n",
    "print(\"Reference image:\", ref_image)\n",
    "print(\"Closest target image:\", tgt_images[0])\n",
    "print(\"Farthest target image:\", tgt_images[1])\n",
    "\n",
    "ref_time = datetime.strptime(os.path.basename(ref_image).split(\"_\")[3], \"%Y%m%d\")\n",
    "tgt_times = [\n",
    "    datetime.strptime(os.path.basename(tgt).split(\"_\")[3], \"%Y%m%d\")\n",
    "    for tgt in tgt_images\n",
    "]\n",
    "print(\"Time differences:\", [(tgt_time - ref_time).days for tgt_time in tgt_times])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == \"LANDSAT_7\" and fill_nodata:\n",
    "    output_dir = f\"data/{inputs_dir}/{dir_suffix}{platform}_{pr}_infilled\"\n",
    "    process_existing_outputs(\n",
    "        [ref_image] + tgt_images,\n",
    "        output_dir,\n",
    "        subdir=subdir,\n",
    "        force_reprocess=force_reprocess,\n",
    "        min_max_scaling=False,\n",
    "        fill_nodata=True,\n",
    "    )\n",
    "\n",
    "    scene_df = pd.read_csv(\n",
    "        f\"data/{inputs_dir}/{dir_suffix}{platform}_{pr}_infilled/pairs.csv\"\n",
    "    )\n",
    "    ref_image_infill = scene_df[\"Reference\"][0]\n",
    "    tgt_images_infill = [\n",
    "        scene_df[\"Closest_target\"][0],\n",
    "        scene_df[\"Farthest_target\"][0],\n",
    "    ]\n",
    "else:\n",
    "    ref_image_infill = ref_image\n",
    "    tgt_images_infill = tgt_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "im0 = rasterio.open(str(ref_image).replace(subdir, f\"{subdir}_ds\"))\n",
    "im1 = rasterio.open(str(tgt_images[0]).replace(subdir, f\"{subdir}_ds\"))\n",
    "im2 = rasterio.open(str(tgt_images[1]).replace(subdir, f\"{subdir}_ds\"))\n",
    "show(im0, ax=axes[0], cmap=\"gray\", title=os.path.basename(ref_image))\n",
    "show(im1, ax=axes[1], cmap=\"gray\", title=os.path.basename(tgt_images[0]))\n",
    "show(im2, ax=axes[2], cmap=\"gray\", title=os.path.basename(tgt_images[1]))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765cff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"data/{inputs_dir}/{dir_suffix}{platform}_{pr}_edge_detection\"\n",
    "if platform == \"LANDSAT_7\" and fill_nodata:\n",
    "    output_dir = (\n",
    "        f\"data/{inputs_dir}/{dir_suffix}{platform}_{pr}_infilled_edge_detection\"\n",
    "    )\n",
    "process_existing_outputs(\n",
    "    [ref_image_infill] + tgt_images_infill,\n",
    "    output_dir,\n",
    "    edge_detection=True,\n",
    "    edge_detection_mode=\"canny\",\n",
    "    subdir=subdir,\n",
    "    force_reprocess=force_reprocess,\n",
    "    fill_nodata=fill_nodata,\n",
    ")\n",
    "\n",
    "scene_df = pd.read_csv(f\"{output_dir}/pairs.csv\")\n",
    "ref_image_edge = scene_df[\"Reference\"][0]\n",
    "tgt_images_edge = [\n",
    "    scene_df[\"Closest_target\"][0],\n",
    "    scene_df[\"Farthest_target\"][0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07023508",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "im0 = rasterio.open(str(ref_image_edge).replace(subdir, f\"{subdir}_ds\"))\n",
    "im1 = rasterio.open(str(tgt_images_edge[0]).replace(subdir, f\"{subdir}_ds\"))\n",
    "im2 = rasterio.open(str(tgt_images_edge[1]).replace(subdir, f\"{subdir}_ds\"))\n",
    "show(im0, ax=axes[0], cmap=\"gray\", title=os.path.basename(ref_image_edge))\n",
    "show(im1, ax=axes[1], cmap=\"gray\", title=os.path.basename(tgt_images_edge[0]))\n",
    "show(im2, ax=axes[2], cmap=\"gray\", title=os.path.basename(tgt_images_edge[1]))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8da42",
   "metadata": {},
   "source": [
    "#### Co_Register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca444c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"data/{outputs_folder}/{dir_suffix}{platform}_{pr}/Co_Register\"\n",
    "\n",
    "shifts, target_ids = coreg(\n",
    "    ref_image_infill,\n",
    "    tgt_images_infill,\n",
    "    output_dir,\n",
    "    # phase_corr_filter=False,\n",
    "    # phase_corr_valid_num_points=1,\n",
    "    # of_dist_thresh=5,\n",
    "    # band_number=2,\n",
    "    # no_ransac=True,\n",
    "    method=\"Co_Register\",\n",
    ")\n",
    "print(\"\\nCo-register shifts:\")\n",
    "for i, shift in enumerate(shifts):\n",
    "    print(\n",
    "        f\"Target {target_ids[i]}: {tuple([np.round(el.tolist(), 3).tolist() for el in shift])} pixels\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde9d27",
   "metadata": {},
   "source": [
    "#### Karios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"data/{outputs_folder}/{dir_suffix}{platform}_{pr}/Karios\"\n",
    "shifts, target_ids = coreg(\n",
    "    ref_image_infill,\n",
    "    tgt_images_infill,\n",
    "    output_dir,\n",
    "    method=\"Karios\",\n",
    "    # scan_big_shifts=True,\n",
    ")\n",
    "print(\"\\nKarios shifts:\")\n",
    "for i, shift in enumerate(shifts):\n",
    "    print(\n",
    "        f\"Target {target_ids[i]}: {tuple([np.round(el, 3).tolist() for el in shifts[shift]])} pixels\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cef64f",
   "metadata": {},
   "source": [
    "#### AROSICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"data/{outputs_folder}/{dir_suffix}{platform}_{pr}/AROSICS\"\n",
    "shifts, target_ids = coreg(\n",
    "    ref_image_infill,\n",
    "    tgt_images_infill,\n",
    "    output_dir,\n",
    "    method=\"AROSICS\",\n",
    "    existing_ref_image=(\n",
    "        ref_image_infill if (platform == \"LANDSAT_7\" and not fill_nodata) else None\n",
    "    ),\n",
    "    existing_tgt_images=(\n",
    "        tgt_images_infill if (platform == \"LANDSAT_7\" and not fill_nodata) else None\n",
    "    ),\n",
    ")\n",
    "print(\"\\nAROSICS shifts:\")\n",
    "for i, shift in enumerate(shifts):\n",
    "    print(\n",
    "        f\"Target {target_ids[i]}: {tuple([np.round(el.tolist(), 3).tolist() for el in shift])} pixels\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188eff14",
   "metadata": {},
   "source": [
    "#### AROSICS EDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7139bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"data/{outputs_folder}/{dir_suffix}{platform}_{pr}/AROSICS_edge\"\n",
    "shifts, target_ids = coreg(\n",
    "    ref_image_edge,\n",
    "    tgt_images_edge,\n",
    "    output_dir,\n",
    "    method=\"AROSICS\",\n",
    "    existing_ref_image=ref_image_infill,\n",
    "    existing_tgt_images=tgt_images_infill,\n",
    ")\n",
    "print(\"\\nAROSICS shifts:\")\n",
    "for i, shift in enumerate(shifts):\n",
    "    print(\n",
    "        f\"Target {target_ids[i]}: {tuple([np.round(el.tolist(), 3).tolist() for el in shift])} pixels\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_output = f\"data/{outputs_folder}/{dir_suffix}{platform}_{pr}\"\n",
    "\n",
    "combine_comparison_results(root_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b14c8b",
   "metadata": {},
   "source": [
    "### Only Landsat 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill_nodata and (platform == \"LANDSAT_7\"):\n",
    "    print(\"\\nProcessing filled nodata outputs for comparison...\")\n",
    "    results_csv = (\n",
    "        f\"data/{outputs_folder}/{dir_suffix}LANDSAT_7_{pr}/co_registration_results.csv\"\n",
    "    )\n",
    "    results_df = pd.read_csv(results_csv)\n",
    "    results_df = results_df.replace(to_replace=\"Failed\", value=(\"50, 50\"))\n",
    "    coreg_shifts = (\n",
    "        results_df[\"Co-Register Shifts\"]\n",
    "        .apply(\n",
    "            lambda x: (\n",
    "                float(x.split(\",\")[0].replace(\"(\", \"\")),\n",
    "                float(x.split(\",\")[1].replace(\")\", \"\")),\n",
    "            )\n",
    "        )\n",
    "        .to_list()\n",
    "    )\n",
    "    karios_shifts = (\n",
    "        results_df[\"Karios Shifts\"]\n",
    "        .apply(\n",
    "            lambda x: (\n",
    "                float(x.split(\",\")[0].replace(\"(\", \"\")),\n",
    "                float(x.split(\",\")[1].replace(\")\", \"\")),\n",
    "            )\n",
    "        )\n",
    "        .to_list()\n",
    "    )\n",
    "    arosics_shifts = (\n",
    "        results_df[\"AROSICS Shifts\"]\n",
    "        .apply(\n",
    "            lambda x: (\n",
    "                float(x.split(\",\")[0].replace(\"(\", \"\")),\n",
    "                float(x.split(\",\")[1].replace(\")\", \"\")),\n",
    "            )\n",
    "        )\n",
    "        .to_list()\n",
    "    )\n",
    "    arosics_edge_shifts = (\n",
    "        results_df[\"AROSICS Edge Shifts\"]\n",
    "        .apply(\n",
    "            lambda x: (\n",
    "                float(x.split(\",\")[0].replace(\"(\", \"\")),\n",
    "                float(x.split(\",\")[1].replace(\")\", \"\")),\n",
    "            )\n",
    "        )\n",
    "        .to_list()\n",
    "    )\n",
    "    print(\"Co-Register shifts:\", coreg_shifts)\n",
    "    print(\"Karios shifts:\", karios_shifts)\n",
    "    print(\"AROSICS shifts:\", arosics_shifts)\n",
    "    print(\"AROSICS Edge shifts:\", arosics_edge_shifts)\n",
    "    tool_names = [\"Co_Register\", \"Karios\", \"AROSICS\", \"AROSICS_edge\"]\n",
    "    tool_shifts = [\n",
    "        coreg_shifts,\n",
    "        karios_shifts,\n",
    "        arosics_shifts,\n",
    "        arosics_edge_shifts,\n",
    "    ]\n",
    "\n",
    "    for tool_name, tool_shift in zip(tool_names, tool_shifts):\n",
    "        output_dir = f\"data/{outputs_folder}_L7/{dir_suffix}{platform}_{pr}/{tool_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(f\"{output_dir}/Aligned\", exist_ok=True)\n",
    "        processed_output_images = []\n",
    "        processed_tgt_images = []\n",
    "        for i, tgt_image in enumerate(tgt_images):\n",
    "            output_path = os.path.join(\n",
    "                f\"{output_dir}/Aligned\", os.path.basename(tgt_image)\n",
    "            )\n",
    "            warp_affine_dataset(\n",
    "                tgt_image,\n",
    "                output_path,\n",
    "                translation_x=tool_shift[i][0],\n",
    "                translation_y=tool_shift[i][1],\n",
    "            )\n",
    "            processed_output_images.append(output_path)\n",
    "            processed_tgt_images.append(tgt_image)\n",
    "        generate_results_from_raw_inputs(\n",
    "            ref_image,\n",
    "            processed_output_images,\n",
    "            processed_tgt_images,\n",
    "            output_dir=output_dir,\n",
    "            shifts=np.array(tool_shift),\n",
    "            run_time=0.0,\n",
    "            target_ids=list(range(len(processed_tgt_images))),\n",
    "        )\n",
    "    root_output = f\"data/{outputs_folder}_L7/{dir_suffix}{platform}_{pr}\"\n",
    "\n",
    "    combine_comparison_results(root_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f2361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
